{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitmandava/Credit-Card-Fraud-Detection/blob/main/Credit_Card_Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ_2Chccsnt0",
        "outputId": "0573beac-8a3d-4585-9f9a-22d61bac1cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow import keras\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTZVY6MFsJCp"
      },
      "source": [
        "# Data Overview and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "XHL1QSRNxmW1",
        "outputId": "3c07ee27-426d-4674-8848-28dc7b0a4bd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ad3bfb9-4b57-44d8-bce1-cf818f4f5bbf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ad3bfb9-4b57-44d8-bce1-cf818f4f5bbf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ad3bfb9-4b57-44d8-bce1-cf818f4f5bbf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ad3bfb9-4b57-44d8-bce1-cf818f4f5bbf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-50331d42-1504-4176-af88-b70707718b41\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-50331d42-1504-4176-af88-b70707718b41')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-50331d42-1504-4176-af88-b70707718b41 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Loading the dataset\n",
        "dataset = pd.read_csv(\"/content/drive/Shareddrives/ML Project/creditcard.csv\")\n",
        "\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6wliXbxMNpt",
        "outputId": "4e4722c3-155c-4e32-c212-0d5669f51547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genuine:  284315\n",
            "Fraud:  492\n",
            "% Genuine Transactions:  99.827251436938\n",
            "% Fraudulent Transactions:  0.1727485630620034\n"
          ]
        }
      ],
      "source": [
        "# Number of Genuine and Fraud point\n",
        "print(\"Genuine: \",len([x for x in dataset[\"Class\"] if x==0]))\n",
        "print(\"Fraud: \",len([x for x in dataset[\"Class\"] if x!=0]))\n",
        "\n",
        "print(\"% Genuine Transactions: \", 100*len([x for x in dataset[\"Class\"] if x==0])/len(dataset))\n",
        "print(\"% Fraudulent Transactions: \", 100*len([x for x in dataset[\"Class\"] if x==1])/len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "OCeUjR1o4f5S",
        "outputId": "5f204e37-d806-41b1-8c4f-52e94103f5c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Frequency')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "amountValues = dataset['Amount'].values\n",
        "timeValues = dataset['Time'].values\n",
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(amountValues, color='r', bins=50)\n",
        "plt.title('Distribution of Transaction Amount', fontsize=14)\n",
        "plt.xlabel('Amount')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(timeValues, color='b', bins=50)\n",
        "plt.title('Distribution of Transaction Time', fontsize=14)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Frequency')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9e-Gl7AE4xX_"
      },
      "outputs": [],
      "source": [
        "robustScaler = RobustScaler()\n",
        "\n",
        "dataset['scaled_amount'] = robustScaler.fit_transform(dataset['Amount'].values.reshape(-1,1))\n",
        "dataset['scaled_time'] = robustScaler.fit_transform(dataset['Time'].values.reshape(-1,1))\n",
        "\n",
        "dataset.drop(['Time','Amount'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1eq_OS3GLCEg"
      },
      "outputs": [],
      "source": [
        "# Train Test split\n",
        "features = list(dataset.columns)\n",
        "features.remove(\"Class\")\n",
        "labels = [\"Class\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset[features], dataset[labels],\n",
        "                                                    train_size=0.75,\n",
        "                                                    #stratify=dataset[labels]\n",
        "                                                    random_state=42)\n",
        "X_train, X_vald, y_train, y_vald = train_test_split(X_train, y_train,\n",
        "                                                    train_size=0.8,\n",
        "                                                    # stratify=y_train,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn2qMZ2YrCS-",
        "outputId": "ecc8969a-68d5-46fd-ead6-3de145252677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "284315\n",
            "492\n"
          ]
        }
      ],
      "source": [
        "fraudPoints = dataset[dataset[\"Class\"] == 1]\n",
        "restOfPoints = dataset[dataset[\"Class\"] == 0]\n",
        "# print(restOfPoints)\n",
        "\n",
        "print(len(restOfPoints))\n",
        "print(len(fraudPoints))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_score(label, prediction, train=True):\n",
        "    if train:\n",
        "        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n",
        "        print(\"Train Result:\\n================================================\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Classification Report:\\n{clf_report}\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, prediction)}\\n\")\n",
        "\n",
        "    elif train==False:\n",
        "        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n",
        "        print(\"Test Result:\\n================================================\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Classification Report:\\n{clf_report}\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Confusion Matrix: \\n {confusion_matrix(label, prediction)}\\n\")"
      ],
      "metadata": {
        "id": "wf8sJabhb5YW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8_qUD4Z2oe2"
      },
      "source": [
        "# Basic Classification Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OXN8kuDsJgy"
      },
      "source": [
        "## K-NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMOdaXYssnVO",
        "outputId": "cf3174de-aef5-4515-b32e-d6ca454711a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        }
      ],
      "source": [
        "#Create an instance of k nearest neighbor classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# #fit the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# #Predict the labels for train data\n",
        "knn_pred_train = knn.predict(X_train)\n",
        "\n",
        "# #Predict the labels for test data\n",
        "knn_pred_test = knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00kLPFRC5Pyr",
        "outputId": "e56f43d0-71cd-4fc2-9673-9ab6d0cadf2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Training Data>\n",
            "Accuracy:\n",
            "0.9995318461646497\n",
            "Precision:\n",
            "0.9703977635689127\n",
            "Recall:\n",
            "0.8933986550532473 \n",
            "\n",
            "\n",
            "<Test Data>\n",
            "Accuracy:\n",
            "0.9993679952810315\n",
            "Precision:\n",
            "0.9268653489157033\n",
            "Recall:\n",
            "0.8627333902896495\n"
          ]
        }
      ],
      "source": [
        "print(\"<Training Data>\")\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(y_train, knn_pred_train))\n",
        "print(\"Precision:\")\n",
        "print(precision_score(y_train, knn_pred_train, average='macro'))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_train, knn_pred_train, average='macro'),\"\\n\\n\")\n",
        "print(\"<Test Data>\")\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(y_test, knn_pred_test))\n",
        "print(\"Precision:\")\n",
        "print(precision_score(y_test, knn_pred_test, average='macro'))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_test, knn_pred_test, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oquENL75XyI8"
      },
      "outputs": [],
      "source": [
        "# # Defining cross validation parameters for grid search\n",
        "# n_neighbors = [3,5,7,9,11]\n",
        "# weights = ['uniform', 'distance'] # Possible to create own weight metric, look into it\n",
        "# algorithm = ['ball_tree', 'kd_tree', 'brute']\n",
        "# # leaf_size = []\n",
        "# p = [1,2,3,4]\n",
        "\n",
        "# param_grid = {'n_neighbors':n_neighbors, 'weights':weights, 'algorithm':algorithm, 'p':p}\n",
        "\n",
        "# # Running grid search CV\n",
        "# gridSearch_KNN = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, cv = 5, scoring='recall', verbose = 2.2)\n",
        "# gridSearch_KNN.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "964pU_GGsJk8"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcokMeNk1LVg",
        "outputId": "356d5b04-4778-4f7f-d793-8acccb0e05d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "#Create an instance of support vector machine\n",
        "svm = SVC()\n",
        "\n",
        "#fit the model\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "#Predict the labels for train data\n",
        "svm_pred_train = svm.predict(X_train)\n",
        "\n",
        "#Predict the labels for test data\n",
        "svm_pred_test = svm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtAOTu6V4XSL",
        "outputId": "6bb4d48d-a41f-4080-b30e-473c1bb3422d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Training Data>\n",
            "Accuracy:\n",
            "0.9996196250087779\n",
            "Precision:\n",
            "0.9879223514151312\n",
            "Recall:\n",
            "0.9032611013655611 \n",
            "\n",
            "\n",
            "<Test Data>\n",
            "Accuracy:\n",
            "0.9993539507317211\n",
            "Precision:\n",
            "0.951540071053488\n",
            "Recall:\n",
            "0.8318021395839716\n"
          ]
        }
      ],
      "source": [
        "print(\"<Training Data>\")\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(y_train, svm_pred_train))\n",
        "print(\"Precision:\")\n",
        "print(precision_score(y_train, svm_pred_train, average='macro'))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_train, svm_pred_train, average='macro'),\"\\n\\n\")\n",
        "print(\"<Test Data>\")\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(y_test, svm_pred_test))\n",
        "print(\"Precision:\")\n",
        "print(precision_score(y_test, svm_pred_test, average='macro'))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_test, svm_pred_test, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymd6LpyNsJ9B"
      },
      "source": [
        "## Decision Tree Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d60GBn_gS2iO"
      },
      "outputs": [],
      "source": [
        "#Create an instance of decision tree classifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "\n",
        "#fit the model\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "#Predict the labels for train data\n",
        "dtc_pred_train = dtc.predict(X_train)\n",
        "\n",
        "#Predict the labels for test data\n",
        "dtc_pred_test = dtc.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cZ9HZu2ITJv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7653c3-bd51-4dd1-d688-d36316c080a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Training Data>\n",
            "Accuracy:\n",
            "1.0\n",
            "Precision:\n",
            "1.0\n",
            "Recall:\n",
            "1.0 \n",
            "\n",
            "\n",
            "<Test Data>\n",
            "Accuracy:\n",
            "0.9991854161399961\n",
            "Precision:\n",
            "0.873662876701236\n",
            "Recall:\n",
            "0.8670597009332811\n"
          ]
        }
      ],
      "source": [
        "print(\"<Training Data>\")\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(y_train, dtc_pred_train))\n",
        "print(\"Precision:\")\n",
        "print(precision_score(y_train, dtc_pred_train, average='macro'))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_train, dtc_pred_train, average='macro'),\"\\n\\n\")\n",
        "print(\"<Test Data>\")\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(y_test, dtc_pred_test))\n",
        "print(\"Precision:\")\n",
        "print(precision_score(y_test, dtc_pred_test, average='macro'))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_test, dtc_pred_test, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkpO6tI5sKRw"
      },
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SDfECybKLUWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff615e45-b6db-4487-d79c-0f05cafea88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-6bd1c4b81efc>:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  rfc.fit(X_train, y_train)\n"
          ]
        }
      ],
      "source": [
        "#Create an instance of random forest classifier\n",
        "rfc = RandomForestClassifier()\n",
        "\n",
        "# #fit the model\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# # #Predict the labels for train data\n",
        "rfc_pred_train = rfc.predict(X_train)\n",
        "\n",
        "# # #Predict the labels for test data\n",
        "rfc_pred_test = rfc.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "juJzvkF9Suy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef219c8-fcdb-4a85-990f-4a631d5c83ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Training Data>\n",
            "Accuracy:\n",
            "0.9999941480770581\n",
            "\n",
            "Precision:\n",
            "0.9999970688240122\n",
            "Recall:\n",
            "0.9983606557377049\n",
            "\n",
            "\n",
            "<Test Data>\n",
            "Accuracy:\n",
            "0.9994663071262043\n",
            "Precision:\n",
            "0.9431091090060655\n",
            "Recall:\n",
            "0.880453605644775\n"
          ]
        }
      ],
      "source": [
        "print(\"<Training Data>\")\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(y_train, rfc_pred_train))\n",
        "print()\n",
        "print(\"Precision:\")\n",
        "print(precision_score(y_train, rfc_pred_train, average='macro'))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_train, rfc_pred_train, average='macro'))\n",
        "print()\n",
        "print()\n",
        "print(\"<Test Data>\")\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(y_test, rfc_pred_test))\n",
        "print(\"Precision:\")\n",
        "print(precision_score(y_test, rfc_pred_test, average='macro'))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_test, rfc_pred_test, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OQV2AtggVifv"
      },
      "outputs": [],
      "source": [
        "# n_estimators = [100,150,200]\n",
        "# criterion = ['gini', 'entropy', 'log_loss']\n",
        "# max_features = ['sqrt', 'log2']\n",
        "# bootstrap = [True,False]\n",
        "# # oob_score = [True,False]\n",
        "\n",
        "# param_grid = {'criterion': criterion, 'max_features':max_features, 'bootstrap':bootstrap}\n",
        "\n",
        "# gridSearch_RFC= GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid,scoring='recall',cv=5,verbose=3)\n",
        "# gridSearch_RFC.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyCe0Am6sabn"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1YCLgv-GRp1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "960a7a1c-58b3-457f-dbf4-7d5b506ba443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "## create an instance of the logistic regressor\n",
        "lr = LogisticRegression()\n",
        "\n",
        "## Learn the classiifer using the training data\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "#Predict the labels for train data\n",
        "lr_pred_train = lr.predict(X_train)\n",
        "\n",
        "#Predict the labels for test data\n",
        "lr_pred_test = lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5qDXbiEgWozz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e307b00-d27d-4f2f-ec74-98c033133047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Training Data>\n",
            "Accuracy:\n",
            "0.9992860654010908\n",
            "Precision:\n",
            "0.9530374216029236\n",
            "Recall:\n",
            "0.8343646744515953\n",
            "\n",
            "\n",
            "<Test Data>\n",
            "Accuracy:\n",
            "0.9992275497879273\n",
            "Precision:\n",
            "0.9265269623309281\n",
            "Recall:\n",
            "0.8096501120308246\n"
          ]
        }
      ],
      "source": [
        "print(\"<Training Data>\")\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(y_train, lr_pred_train))\n",
        "print(\"Precision:\")\n",
        "print(precision_score(y_train, lr_pred_train, average='macro'))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_train, lr_pred_train, average='macro'))\n",
        "print()\n",
        "print()\n",
        "print(\"<Test Data>\")\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(y_test, lr_pred_test))\n",
        "print(\"Precision:\")\n",
        "print(precision_score(y_test, lr_pred_test, average='macro'))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_test, lr_pred_test, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9XGkPik3b8_s"
      },
      "outputs": [],
      "source": [
        "# # Logistic regression CV\n",
        "\n",
        "# penalty = [\"l2\", 'none']\n",
        "# tol = [1e-5, 1e-4, 1e-3]\n",
        "# C = [0.5, 1, 2]\n",
        "# max_iter = [50,100,200]\n",
        "\n",
        "# param_grid = {\"penalty\" : penalty, \"tol\" : tol, \"C\" : C, \"max_iter\" : max_iter}\n",
        "\n",
        "# gridSearch_LR= GridSearchCV(estimator=LogisticRegression(), param_grid=param_grid,scoring='recall',cv=5,verbose=3)\n",
        "# gridSearch_LR.fit(X_train, y_train)\n",
        "\n",
        "# print()\n",
        "# print(gridSearch_LR.cv_results_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QWon9Uw604r"
      },
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANN"
      ],
      "metadata": {
        "id": "vQGnfVI9besM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "q_OH29CA4Npk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d226d0a7-ac79-4c0b-b873-7e01b30ee9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               7936      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 256)               1024      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 256)               1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 256)               1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 142849 (558.00 KB)\n",
            "Trainable params: 141313 (552.00 KB)\n",
            "Non-trainable params: 1536 (6.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[-1],)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [\n",
        "    keras.metrics.FalseNegatives(name='fn'),\n",
        "    keras.metrics.FalsePositives(name='fp'),\n",
        "    keras.metrics.TrueNegatives(name='tn'),\n",
        "    keras.metrics.TruePositives(name='tp'),\n",
        "    keras.metrics.Precision(name='precision'),\n",
        "    keras.metrics.Recall(name='recall')\n",
        "]\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=metrics)\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint('fraud_model_at_epoch_{epoch}.h5')]\n",
        "# class_weight = {0:w_real, 1:w_fraud}\n",
        "\n",
        "r = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_vald, y_vald),\n",
        "    batch_size=2048,\n",
        "    epochs=300,\n",
        "    # class_weight=class_weight,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L70G9ceWOMl",
        "outputId": "ae2aa7aa-adf9-41b6-c0f8-fcad43caac1f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "84/84 [==============================] - 8s 15ms/step - loss: 0.7996 - fn: 91.0000 - fp: 78647.0000 - tn: 91932.0000 - tp: 214.0000 - precision: 0.0027 - recall: 0.7016 - val_loss: 0.6011 - val_fn: 16.0000 - val_fp: 1659.0000 - val_tn: 40988.0000 - val_tp: 58.0000 - val_precision: 0.0338 - val_recall: 0.7838\n",
            "Epoch 2/300\n",
            "17/84 [=====>........................] - ETA: 0s - loss: 0.7164 - fn: 12.0000 - fp: 14181.0000 - tn: 20580.0000 - tp: 43.0000 - precision: 0.0030 - recall: 0.7818"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/84 [==============================] - 1s 10ms/step - loss: 0.6770 - fn: 52.0000 - fp: 63722.0000 - tn: 106857.0000 - tp: 253.0000 - precision: 0.0040 - recall: 0.8295 - val_loss: 0.5328 - val_fn: 16.0000 - val_fp: 390.0000 - val_tn: 42257.0000 - val_tp: 58.0000 - val_precision: 0.1295 - val_recall: 0.7838\n",
            "Epoch 3/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.5980 - fn: 40.0000 - fp: 48445.0000 - tn: 122134.0000 - tp: 265.0000 - precision: 0.0054 - recall: 0.8689 - val_loss: 0.4837 - val_fn: 16.0000 - val_fp: 123.0000 - val_tn: 42524.0000 - val_tp: 58.0000 - val_precision: 0.3204 - val_recall: 0.7838\n",
            "Epoch 4/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.5228 - fn: 46.0000 - fp: 32996.0000 - tn: 137583.0000 - tp: 259.0000 - precision: 0.0078 - recall: 0.8492 - val_loss: 0.4122 - val_fn: 15.0000 - val_fp: 38.0000 - val_tn: 42609.0000 - val_tp: 59.0000 - val_precision: 0.6082 - val_recall: 0.7973\n",
            "Epoch 5/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.4540 - fn: 50.0000 - fp: 20646.0000 - tn: 149933.0000 - tp: 255.0000 - precision: 0.0122 - recall: 0.8361 - val_loss: 0.3568 - val_fn: 15.0000 - val_fp: 12.0000 - val_tn: 42635.0000 - val_tp: 59.0000 - val_precision: 0.8310 - val_recall: 0.7973\n",
            "Epoch 6/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.3836 - fn: 53.0000 - fp: 11456.0000 - tn: 159123.0000 - tp: 252.0000 - precision: 0.0215 - recall: 0.8262 - val_loss: 0.3011 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 58.0000 - val_precision: 0.8529 - val_recall: 0.7838\n",
            "Epoch 7/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.3202 - fn: 59.0000 - fp: 6070.0000 - tn: 164509.0000 - tp: 246.0000 - precision: 0.0389 - recall: 0.8066 - val_loss: 0.2439 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 58.0000 - val_precision: 0.8529 - val_recall: 0.7838\n",
            "Epoch 8/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.2661 - fn: 59.0000 - fp: 3372.0000 - tn: 167207.0000 - tp: 246.0000 - precision: 0.0680 - recall: 0.8066 - val_loss: 0.2057 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 58.0000 - val_precision: 0.8529 - val_recall: 0.7838\n",
            "Epoch 9/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.2195 - fn: 62.0000 - fp: 2051.0000 - tn: 168528.0000 - tp: 243.0000 - precision: 0.1059 - recall: 0.7967 - val_loss: 0.1660 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 58.0000 - val_precision: 0.8529 - val_recall: 0.7838\n",
            "Epoch 10/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.1764 - fn: 61.0000 - fp: 1292.0000 - tn: 169287.0000 - tp: 244.0000 - precision: 0.1589 - recall: 0.8000 - val_loss: 0.1377 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 58.0000 - val_precision: 0.8529 - val_recall: 0.7838\n",
            "Epoch 11/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 0.1456 - fn: 66.0000 - fp: 866.0000 - tn: 169713.0000 - tp: 239.0000 - precision: 0.2163 - recall: 0.7836 - val_loss: 0.1157 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 12/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.1188 - fn: 71.0000 - fp: 608.0000 - tn: 169971.0000 - tp: 234.0000 - precision: 0.2779 - recall: 0.7672 - val_loss: 0.0923 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 13/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.1000 - fn: 83.0000 - fp: 550.0000 - tn: 170029.0000 - tp: 222.0000 - precision: 0.2876 - recall: 0.7279 - val_loss: 0.0784 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 14/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0857 - fn: 86.0000 - fp: 450.0000 - tn: 170129.0000 - tp: 219.0000 - precision: 0.3274 - recall: 0.7180 - val_loss: 0.0676 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 15/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0732 - fn: 89.0000 - fp: 331.0000 - tn: 170248.0000 - tp: 216.0000 - precision: 0.3949 - recall: 0.7082 - val_loss: 0.0568 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 16/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0607 - fn: 83.0000 - fp: 308.0000 - tn: 170271.0000 - tp: 222.0000 - precision: 0.4189 - recall: 0.7279 - val_loss: 0.0491 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 17/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0554 - fn: 95.0000 - fp: 235.0000 - tn: 170344.0000 - tp: 210.0000 - precision: 0.4719 - recall: 0.6885 - val_loss: 0.0432 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 18/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0471 - fn: 90.0000 - fp: 194.0000 - tn: 170385.0000 - tp: 215.0000 - precision: 0.5257 - recall: 0.7049 - val_loss: 0.0382 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 19/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0423 - fn: 99.0000 - fp: 195.0000 - tn: 170384.0000 - tp: 206.0000 - precision: 0.5137 - recall: 0.6754 - val_loss: 0.0353 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 20/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0376 - fn: 94.0000 - fp: 170.0000 - tn: 170409.0000 - tp: 211.0000 - precision: 0.5538 - recall: 0.6918 - val_loss: 0.0314 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 21/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0343 - fn: 92.0000 - fp: 141.0000 - tn: 170438.0000 - tp: 213.0000 - precision: 0.6017 - recall: 0.6984 - val_loss: 0.0285 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 22/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0306 - fn: 89.0000 - fp: 137.0000 - tn: 170442.0000 - tp: 216.0000 - precision: 0.6119 - recall: 0.7082 - val_loss: 0.0256 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 23/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0287 - fn: 92.0000 - fp: 121.0000 - tn: 170458.0000 - tp: 213.0000 - precision: 0.6377 - recall: 0.6984 - val_loss: 0.0236 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 24/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0252 - fn: 86.0000 - fp: 120.0000 - tn: 170459.0000 - tp: 219.0000 - precision: 0.6460 - recall: 0.7180 - val_loss: 0.0217 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 25/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0229 - fn: 91.0000 - fp: 95.0000 - tn: 170484.0000 - tp: 214.0000 - precision: 0.6926 - recall: 0.7016 - val_loss: 0.0194 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 26/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0223 - fn: 99.0000 - fp: 113.0000 - tn: 170466.0000 - tp: 206.0000 - precision: 0.6458 - recall: 0.6754 - val_loss: 0.0197 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 27/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0201 - fn: 84.0000 - fp: 85.0000 - tn: 170494.0000 - tp: 221.0000 - precision: 0.7222 - recall: 0.7246 - val_loss: 0.0182 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 28/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0185 - fn: 85.0000 - fp: 87.0000 - tn: 170492.0000 - tp: 220.0000 - precision: 0.7166 - recall: 0.7213 - val_loss: 0.0175 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 58.0000 - val_precision: 0.8529 - val_recall: 0.7838\n",
            "Epoch 29/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0185 - fn: 89.0000 - fp: 85.0000 - tn: 170494.0000 - tp: 216.0000 - precision: 0.7176 - recall: 0.7082 - val_loss: 0.0160 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 30/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0173 - fn: 90.0000 - fp: 90.0000 - tn: 170489.0000 - tp: 215.0000 - precision: 0.7049 - recall: 0.7049 - val_loss: 0.0146 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 31/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0159 - fn: 93.0000 - fp: 63.0000 - tn: 170516.0000 - tp: 212.0000 - precision: 0.7709 - recall: 0.6951 - val_loss: 0.0142 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 32/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0154 - fn: 88.0000 - fp: 69.0000 - tn: 170510.0000 - tp: 217.0000 - precision: 0.7587 - recall: 0.7115 - val_loss: 0.0134 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 33/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0158 - fn: 100.0000 - fp: 70.0000 - tn: 170509.0000 - tp: 205.0000 - precision: 0.7455 - recall: 0.6721 - val_loss: 0.0126 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 58.0000 - val_precision: 0.8529 - val_recall: 0.7838\n",
            "Epoch 34/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0138 - fn: 98.0000 - fp: 60.0000 - tn: 170519.0000 - tp: 207.0000 - precision: 0.7753 - recall: 0.6787 - val_loss: 0.0128 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 58.0000 - val_precision: 0.8529 - val_recall: 0.7838\n",
            "Epoch 35/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0123 - fn: 88.0000 - fp: 63.0000 - tn: 170516.0000 - tp: 217.0000 - precision: 0.7750 - recall: 0.7115 - val_loss: 0.0112 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 58.0000 - val_precision: 0.8529 - val_recall: 0.7838\n",
            "Epoch 36/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0120 - fn: 97.0000 - fp: 57.0000 - tn: 170522.0000 - tp: 208.0000 - precision: 0.7849 - recall: 0.6820 - val_loss: 0.0111 - val_fn: 16.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 58.0000 - val_precision: 0.8529 - val_recall: 0.7838\n",
            "Epoch 37/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0113 - fn: 92.0000 - fp: 62.0000 - tn: 170517.0000 - tp: 213.0000 - precision: 0.7745 - recall: 0.6984 - val_loss: 0.0104 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 38/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0113 - fn: 93.0000 - fp: 55.0000 - tn: 170524.0000 - tp: 212.0000 - precision: 0.7940 - recall: 0.6951 - val_loss: 0.0099 - val_fn: 18.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 56.0000 - val_precision: 0.8485 - val_recall: 0.7568\n",
            "Epoch 39/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0102 - fn: 105.0000 - fp: 55.0000 - tn: 170524.0000 - tp: 200.0000 - precision: 0.7843 - recall: 0.6557 - val_loss: 0.0100 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 40/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0108 - fn: 89.0000 - fp: 53.0000 - tn: 170526.0000 - tp: 216.0000 - precision: 0.8030 - recall: 0.7082 - val_loss: 0.0096 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 41/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0093 - fn: 91.0000 - fp: 53.0000 - tn: 170526.0000 - tp: 214.0000 - precision: 0.8015 - recall: 0.7016 - val_loss: 0.0094 - val_fn: 18.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 56.0000 - val_precision: 0.8485 - val_recall: 0.7568\n",
            "Epoch 42/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0097 - fn: 91.0000 - fp: 51.0000 - tn: 170528.0000 - tp: 214.0000 - precision: 0.8075 - recall: 0.7016 - val_loss: 0.0092 - val_fn: 17.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 57.0000 - val_precision: 0.8636 - val_recall: 0.7703\n",
            "Epoch 43/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0088 - fn: 90.0000 - fp: 48.0000 - tn: 170531.0000 - tp: 215.0000 - precision: 0.8175 - recall: 0.7049 - val_loss: 0.0089 - val_fn: 18.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 56.0000 - val_precision: 0.8615 - val_recall: 0.7568\n",
            "Epoch 44/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0086 - fn: 90.0000 - fp: 45.0000 - tn: 170534.0000 - tp: 215.0000 - precision: 0.8269 - recall: 0.7049 - val_loss: 0.0086 - val_fn: 18.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 56.0000 - val_precision: 0.8615 - val_recall: 0.7568\n",
            "Epoch 45/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0086 - fn: 100.0000 - fp: 46.0000 - tn: 170533.0000 - tp: 205.0000 - precision: 0.8167 - recall: 0.6721 - val_loss: 0.0090 - val_fn: 17.0000 - val_fp: 10.0000 - val_tn: 42637.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.7703\n",
            "Epoch 46/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0078 - fn: 93.0000 - fp: 57.0000 - tn: 170522.0000 - tp: 212.0000 - precision: 0.7881 - recall: 0.6951 - val_loss: 0.0079 - val_fn: 18.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 56.0000 - val_precision: 0.8615 - val_recall: 0.7568\n",
            "Epoch 47/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0081 - fn: 99.0000 - fp: 41.0000 - tn: 170538.0000 - tp: 206.0000 - precision: 0.8340 - recall: 0.6754 - val_loss: 0.0079 - val_fn: 18.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 56.0000 - val_precision: 0.8615 - val_recall: 0.7568\n",
            "Epoch 48/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0070 - fn: 93.0000 - fp: 38.0000 - tn: 170541.0000 - tp: 212.0000 - precision: 0.8480 - recall: 0.6951 - val_loss: 0.0078 - val_fn: 18.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 56.0000 - val_precision: 0.8615 - val_recall: 0.7568\n",
            "Epoch 49/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0073 - fn: 101.0000 - fp: 40.0000 - tn: 170539.0000 - tp: 204.0000 - precision: 0.8361 - recall: 0.6689 - val_loss: 0.0075 - val_fn: 19.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 55.0000 - val_precision: 0.8730 - val_recall: 0.7432\n",
            "Epoch 50/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0069 - fn: 99.0000 - fp: 46.0000 - tn: 170533.0000 - tp: 206.0000 - precision: 0.8175 - recall: 0.6754 - val_loss: 0.0075 - val_fn: 19.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 55.0000 - val_precision: 0.8594 - val_recall: 0.7432\n",
            "Epoch 51/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0066 - fn: 94.0000 - fp: 38.0000 - tn: 170541.0000 - tp: 211.0000 - precision: 0.8474 - recall: 0.6918 - val_loss: 0.0071 - val_fn: 19.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 55.0000 - val_precision: 0.8730 - val_recall: 0.7432\n",
            "Epoch 52/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0067 - fn: 100.0000 - fp: 41.0000 - tn: 170538.0000 - tp: 205.0000 - precision: 0.8333 - recall: 0.6721 - val_loss: 0.0072 - val_fn: 19.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 55.0000 - val_precision: 0.8730 - val_recall: 0.7432\n",
            "Epoch 53/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0062 - fn: 96.0000 - fp: 38.0000 - tn: 170541.0000 - tp: 209.0000 - precision: 0.8462 - recall: 0.6852 - val_loss: 0.0073 - val_fn: 19.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 55.0000 - val_precision: 0.8594 - val_recall: 0.7432\n",
            "Epoch 54/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0056 - fn: 96.0000 - fp: 34.0000 - tn: 170545.0000 - tp: 209.0000 - precision: 0.8601 - recall: 0.6852 - val_loss: 0.0068 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 55/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0053 - fn: 89.0000 - fp: 35.0000 - tn: 170544.0000 - tp: 216.0000 - precision: 0.8606 - recall: 0.7082 - val_loss: 0.0064 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 56/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0055 - fn: 92.0000 - fp: 28.0000 - tn: 170551.0000 - tp: 213.0000 - precision: 0.8838 - recall: 0.6984 - val_loss: 0.0065 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 57/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0058 - fn: 101.0000 - fp: 33.0000 - tn: 170546.0000 - tp: 204.0000 - precision: 0.8608 - recall: 0.6689 - val_loss: 0.0064 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 58/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0054 - fn: 98.0000 - fp: 31.0000 - tn: 170548.0000 - tp: 207.0000 - precision: 0.8697 - recall: 0.6787 - val_loss: 0.0063 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 59/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0053 - fn: 99.0000 - fp: 30.0000 - tn: 170549.0000 - tp: 206.0000 - precision: 0.8729 - recall: 0.6754 - val_loss: 0.0063 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 60/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0050 - fn: 84.0000 - fp: 31.0000 - tn: 170548.0000 - tp: 221.0000 - precision: 0.8770 - recall: 0.7246 - val_loss: 0.0061 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 61/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 0.0046 - fn: 91.0000 - fp: 33.0000 - tn: 170546.0000 - tp: 214.0000 - precision: 0.8664 - recall: 0.7016 - val_loss: 0.0063 - val_fn: 19.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 55.0000 - val_precision: 0.8594 - val_recall: 0.7432\n",
            "Epoch 62/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0050 - fn: 93.0000 - fp: 34.0000 - tn: 170545.0000 - tp: 212.0000 - precision: 0.8618 - recall: 0.6951 - val_loss: 0.0060 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 63/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0049 - fn: 97.0000 - fp: 33.0000 - tn: 170546.0000 - tp: 208.0000 - precision: 0.8631 - recall: 0.6820 - val_loss: 0.0060 - val_fn: 19.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 55.0000 - val_precision: 0.8594 - val_recall: 0.7432\n",
            "Epoch 64/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0047 - fn: 93.0000 - fp: 26.0000 - tn: 170553.0000 - tp: 212.0000 - precision: 0.8908 - recall: 0.6951 - val_loss: 0.0059 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 65/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0046 - fn: 94.0000 - fp: 32.0000 - tn: 170547.0000 - tp: 211.0000 - precision: 0.8683 - recall: 0.6918 - val_loss: 0.0057 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 66/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0043 - fn: 83.0000 - fp: 35.0000 - tn: 170544.0000 - tp: 222.0000 - precision: 0.8638 - recall: 0.7279 - val_loss: 0.0055 - val_fn: 20.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 54.0000 - val_precision: 0.9000 - val_recall: 0.7297\n",
            "Epoch 67/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0047 - fn: 91.0000 - fp: 27.0000 - tn: 170552.0000 - tp: 214.0000 - precision: 0.8880 - recall: 0.7016 - val_loss: 0.0056 - val_fn: 19.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 55.0000 - val_precision: 0.8594 - val_recall: 0.7432\n",
            "Epoch 68/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0044 - fn: 88.0000 - fp: 28.0000 - tn: 170551.0000 - tp: 217.0000 - precision: 0.8857 - recall: 0.7115 - val_loss: 0.0054 - val_fn: 20.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.7297\n",
            "Epoch 69/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0045 - fn: 88.0000 - fp: 27.0000 - tn: 170552.0000 - tp: 217.0000 - precision: 0.8893 - recall: 0.7115 - val_loss: 0.0057 - val_fn: 19.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 55.0000 - val_precision: 0.8730 - val_recall: 0.7432\n",
            "Epoch 70/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0044 - fn: 94.0000 - fp: 32.0000 - tn: 170547.0000 - tp: 211.0000 - precision: 0.8683 - recall: 0.6918 - val_loss: 0.0056 - val_fn: 18.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.7568\n",
            "Epoch 71/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0040 - fn: 86.0000 - fp: 27.0000 - tn: 170552.0000 - tp: 219.0000 - precision: 0.8902 - recall: 0.7180 - val_loss: 0.0056 - val_fn: 19.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 55.0000 - val_precision: 0.8594 - val_recall: 0.7432\n",
            "Epoch 72/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0042 - fn: 83.0000 - fp: 28.0000 - tn: 170551.0000 - tp: 222.0000 - precision: 0.8880 - recall: 0.7279 - val_loss: 0.0056 - val_fn: 19.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 55.0000 - val_precision: 0.8594 - val_recall: 0.7432\n",
            "Epoch 73/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0042 - fn: 76.0000 - fp: 25.0000 - tn: 170554.0000 - tp: 229.0000 - precision: 0.9016 - recall: 0.7508 - val_loss: 0.0057 - val_fn: 19.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 55.0000 - val_precision: 0.8594 - val_recall: 0.7432\n",
            "Epoch 74/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0038 - fn: 81.0000 - fp: 22.0000 - tn: 170557.0000 - tp: 224.0000 - precision: 0.9106 - recall: 0.7344 - val_loss: 0.0055 - val_fn: 20.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 54.0000 - val_precision: 0.8852 - val_recall: 0.7297\n",
            "Epoch 75/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0039 - fn: 91.0000 - fp: 23.0000 - tn: 170556.0000 - tp: 214.0000 - precision: 0.9030 - recall: 0.7016 - val_loss: 0.0053 - val_fn: 20.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 54.0000 - val_precision: 0.9000 - val_recall: 0.7297\n",
            "Epoch 76/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0040 - fn: 91.0000 - fp: 33.0000 - tn: 170546.0000 - tp: 214.0000 - precision: 0.8664 - recall: 0.7016 - val_loss: 0.0050 - val_fn: 20.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 54.0000 - val_precision: 0.8852 - val_recall: 0.7297\n",
            "Epoch 77/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0035 - fn: 86.0000 - fp: 23.0000 - tn: 170556.0000 - tp: 219.0000 - precision: 0.9050 - recall: 0.7180 - val_loss: 0.0052 - val_fn: 19.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 55.0000 - val_precision: 0.8730 - val_recall: 0.7432\n",
            "Epoch 78/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0036 - fn: 79.0000 - fp: 25.0000 - tn: 170554.0000 - tp: 226.0000 - precision: 0.9004 - recall: 0.7410 - val_loss: 0.0050 - val_fn: 20.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 54.0000 - val_precision: 0.8852 - val_recall: 0.7297\n",
            "Epoch 79/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0034 - fn: 79.0000 - fp: 26.0000 - tn: 170553.0000 - tp: 226.0000 - precision: 0.8968 - recall: 0.7410 - val_loss: 0.0050 - val_fn: 19.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 55.0000 - val_precision: 0.8730 - val_recall: 0.7432\n",
            "Epoch 80/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0037 - fn: 84.0000 - fp: 22.0000 - tn: 170557.0000 - tp: 221.0000 - precision: 0.9095 - recall: 0.7246 - val_loss: 0.0050 - val_fn: 18.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.7568\n",
            "Epoch 81/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0036 - fn: 81.0000 - fp: 32.0000 - tn: 170547.0000 - tp: 224.0000 - precision: 0.8750 - recall: 0.7344 - val_loss: 0.0051 - val_fn: 18.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.7568\n",
            "Epoch 82/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0035 - fn: 78.0000 - fp: 26.0000 - tn: 170553.0000 - tp: 227.0000 - precision: 0.8972 - recall: 0.7443 - val_loss: 0.0049 - val_fn: 19.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.7432\n",
            "Epoch 83/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0036 - fn: 87.0000 - fp: 34.0000 - tn: 170545.0000 - tp: 218.0000 - precision: 0.8651 - recall: 0.7148 - val_loss: 0.0048 - val_fn: 20.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 54.0000 - val_precision: 0.8852 - val_recall: 0.7297\n",
            "Epoch 84/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0033 - fn: 79.0000 - fp: 24.0000 - tn: 170555.0000 - tp: 226.0000 - precision: 0.9040 - recall: 0.7410 - val_loss: 0.0050 - val_fn: 19.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 55.0000 - val_precision: 0.8730 - val_recall: 0.7432\n",
            "Epoch 85/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0032 - fn: 80.0000 - fp: 20.0000 - tn: 170559.0000 - tp: 225.0000 - precision: 0.9184 - recall: 0.7377 - val_loss: 0.0048 - val_fn: 17.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.7703\n",
            "Epoch 86/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0033 - fn: 79.0000 - fp: 26.0000 - tn: 170553.0000 - tp: 226.0000 - precision: 0.8968 - recall: 0.7410 - val_loss: 0.0048 - val_fn: 19.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 55.0000 - val_precision: 0.8730 - val_recall: 0.7432\n",
            "Epoch 87/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0031 - fn: 76.0000 - fp: 25.0000 - tn: 170554.0000 - tp: 229.0000 - precision: 0.9016 - recall: 0.7508 - val_loss: 0.0048 - val_fn: 17.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.7703\n",
            "Epoch 88/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0035 - fn: 83.0000 - fp: 27.0000 - tn: 170552.0000 - tp: 222.0000 - precision: 0.8916 - recall: 0.7279 - val_loss: 0.0046 - val_fn: 19.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.7432\n",
            "Epoch 89/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0031 - fn: 83.0000 - fp: 24.0000 - tn: 170555.0000 - tp: 222.0000 - precision: 0.9024 - recall: 0.7279 - val_loss: 0.0048 - val_fn: 17.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.7703\n",
            "Epoch 90/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0032 - fn: 78.0000 - fp: 28.0000 - tn: 170551.0000 - tp: 227.0000 - precision: 0.8902 - recall: 0.7443 - val_loss: 0.0048 - val_fn: 17.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.7703\n",
            "Epoch 91/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0031 - fn: 84.0000 - fp: 25.0000 - tn: 170554.0000 - tp: 221.0000 - precision: 0.8984 - recall: 0.7246 - val_loss: 0.0046 - val_fn: 19.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.7432\n",
            "Epoch 92/300\n",
            "84/84 [==============================] - 1s 11ms/step - loss: 0.0032 - fn: 82.0000 - fp: 28.0000 - tn: 170551.0000 - tp: 223.0000 - precision: 0.8884 - recall: 0.7311 - val_loss: 0.0047 - val_fn: 20.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 54.0000 - val_precision: 0.8852 - val_recall: 0.7297\n",
            "Epoch 93/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0030 - fn: 75.0000 - fp: 23.0000 - tn: 170556.0000 - tp: 230.0000 - precision: 0.9091 - recall: 0.7541 - val_loss: 0.0048 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.7568\n",
            "Epoch 94/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0028 - fn: 73.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 232.0000 - precision: 0.9243 - recall: 0.7607 - val_loss: 0.0051 - val_fn: 18.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.7568\n",
            "Epoch 95/300\n",
            "84/84 [==============================] - 1s 11ms/step - loss: 0.0030 - fn: 73.0000 - fp: 28.0000 - tn: 170551.0000 - tp: 232.0000 - precision: 0.8923 - recall: 0.7607 - val_loss: 0.0046 - val_fn: 19.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.7432\n",
            "Epoch 96/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0030 - fn: 82.0000 - fp: 27.0000 - tn: 170552.0000 - tp: 223.0000 - precision: 0.8920 - recall: 0.7311 - val_loss: 0.0046 - val_fn: 18.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.7568\n",
            "Epoch 97/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0029 - fn: 86.0000 - fp: 20.0000 - tn: 170559.0000 - tp: 219.0000 - precision: 0.9163 - recall: 0.7180 - val_loss: 0.0049 - val_fn: 17.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.7703\n",
            "Epoch 98/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0028 - fn: 67.0000 - fp: 24.0000 - tn: 170555.0000 - tp: 238.0000 - precision: 0.9084 - recall: 0.7803 - val_loss: 0.0046 - val_fn: 18.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.7568\n",
            "Epoch 99/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0029 - fn: 78.0000 - fp: 25.0000 - tn: 170554.0000 - tp: 227.0000 - precision: 0.9008 - recall: 0.7443 - val_loss: 0.0047 - val_fn: 18.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.7568\n",
            "Epoch 100/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0029 - fn: 79.0000 - fp: 25.0000 - tn: 170554.0000 - tp: 226.0000 - precision: 0.9004 - recall: 0.7410 - val_loss: 0.0048 - val_fn: 17.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.7703\n",
            "Epoch 101/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0026 - fn: 69.0000 - fp: 30.0000 - tn: 170549.0000 - tp: 236.0000 - precision: 0.8872 - recall: 0.7738 - val_loss: 0.0047 - val_fn: 19.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.7432\n",
            "Epoch 102/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0028 - fn: 80.0000 - fp: 25.0000 - tn: 170554.0000 - tp: 225.0000 - precision: 0.9000 - recall: 0.7377 - val_loss: 0.0047 - val_fn: 19.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.7432\n",
            "Epoch 103/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0028 - fn: 78.0000 - fp: 21.0000 - tn: 170558.0000 - tp: 227.0000 - precision: 0.9153 - recall: 0.7443 - val_loss: 0.0049 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.7568\n",
            "Epoch 104/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0030 - fn: 77.0000 - fp: 22.0000 - tn: 170557.0000 - tp: 228.0000 - precision: 0.9120 - recall: 0.7475 - val_loss: 0.0047 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.7568\n",
            "Epoch 105/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0027 - fn: 68.0000 - fp: 23.0000 - tn: 170556.0000 - tp: 237.0000 - precision: 0.9115 - recall: 0.7770 - val_loss: 0.0046 - val_fn: 19.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.7432\n",
            "Epoch 106/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0027 - fn: 69.0000 - fp: 24.0000 - tn: 170555.0000 - tp: 236.0000 - precision: 0.9077 - recall: 0.7738 - val_loss: 0.0046 - val_fn: 19.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.7432\n",
            "Epoch 107/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0027 - fn: 74.0000 - fp: 25.0000 - tn: 170554.0000 - tp: 231.0000 - precision: 0.9023 - recall: 0.7574 - val_loss: 0.0045 - val_fn: 19.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.7432\n",
            "Epoch 108/300\n",
            "84/84 [==============================] - 1s 11ms/step - loss: 0.0025 - fn: 73.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 232.0000 - precision: 0.9243 - recall: 0.7607 - val_loss: 0.0046 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.7568\n",
            "Epoch 109/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0025 - fn: 71.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 234.0000 - precision: 0.9512 - recall: 0.7672 - val_loss: 0.0043 - val_fn: 19.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.7432\n",
            "Epoch 110/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0024 - fn: 63.0000 - fp: 20.0000 - tn: 170559.0000 - tp: 242.0000 - precision: 0.9237 - recall: 0.7934 - val_loss: 0.0046 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.7568\n",
            "Epoch 111/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0025 - fn: 63.0000 - fp: 21.0000 - tn: 170558.0000 - tp: 242.0000 - precision: 0.9202 - recall: 0.7934 - val_loss: 0.0046 - val_fn: 19.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.7432\n",
            "Epoch 112/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0024 - fn: 68.0000 - fp: 25.0000 - tn: 170554.0000 - tp: 237.0000 - precision: 0.9046 - recall: 0.7770 - val_loss: 0.0046 - val_fn: 18.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.7568\n",
            "Epoch 113/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0025 - fn: 75.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 230.0000 - precision: 0.9350 - recall: 0.7541 - val_loss: 0.0052 - val_fn: 18.0000 - val_fp: 9.0000 - val_tn: 42638.0000 - val_tp: 56.0000 - val_precision: 0.8615 - val_recall: 0.7568\n",
            "Epoch 114/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0027 - fn: 66.0000 - fp: 25.0000 - tn: 170554.0000 - tp: 239.0000 - precision: 0.9053 - recall: 0.7836 - val_loss: 0.0047 - val_fn: 18.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.7568\n",
            "Epoch 115/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0024 - fn: 70.0000 - fp: 17.0000 - tn: 170562.0000 - tp: 235.0000 - precision: 0.9325 - recall: 0.7705 - val_loss: 0.0045 - val_fn: 18.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.7568\n",
            "Epoch 116/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0024 - fn: 66.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 239.0000 - precision: 0.9300 - recall: 0.7836 - val_loss: 0.0044 - val_fn: 18.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.7568\n",
            "Epoch 117/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0024 - fn: 68.0000 - fp: 20.0000 - tn: 170559.0000 - tp: 237.0000 - precision: 0.9222 - recall: 0.7770 - val_loss: 0.0045 - val_fn: 18.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.7568\n",
            "Epoch 118/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0024 - fn: 65.0000 - fp: 22.0000 - tn: 170557.0000 - tp: 240.0000 - precision: 0.9160 - recall: 0.7869 - val_loss: 0.0043 - val_fn: 18.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.7568\n",
            "Epoch 119/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0023 - fn: 63.0000 - fp: 17.0000 - tn: 170562.0000 - tp: 242.0000 - precision: 0.9344 - recall: 0.7934 - val_loss: 0.0044 - val_fn: 17.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 57.0000 - val_precision: 0.9048 - val_recall: 0.7703\n",
            "Epoch 120/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0025 - fn: 71.0000 - fp: 27.0000 - tn: 170552.0000 - tp: 234.0000 - precision: 0.8966 - recall: 0.7672 - val_loss: 0.0047 - val_fn: 18.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.7568\n",
            "Epoch 121/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0024 - fn: 65.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 240.0000 - precision: 0.9412 - recall: 0.7869 - val_loss: 0.0045 - val_fn: 17.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.7703\n",
            "Epoch 122/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0025 - fn: 64.0000 - fp: 21.0000 - tn: 170558.0000 - tp: 241.0000 - precision: 0.9198 - recall: 0.7902 - val_loss: 0.0044 - val_fn: 19.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.7432\n",
            "Epoch 123/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0022 - fn: 71.0000 - fp: 14.0000 - tn: 170565.0000 - tp: 234.0000 - precision: 0.9435 - recall: 0.7672 - val_loss: 0.0047 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 58.0000 - val_precision: 0.8788 - val_recall: 0.7838\n",
            "Epoch 124/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0022 - fn: 59.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 246.0000 - precision: 0.9389 - recall: 0.8066 - val_loss: 0.0044 - val_fn: 18.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.7568\n",
            "Epoch 125/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0025 - fn: 70.0000 - fp: 26.0000 - tn: 170553.0000 - tp: 235.0000 - precision: 0.9004 - recall: 0.7705 - val_loss: 0.0046 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.7568\n",
            "Epoch 126/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0022 - fn: 64.0000 - fp: 17.0000 - tn: 170562.0000 - tp: 241.0000 - precision: 0.9341 - recall: 0.7902 - val_loss: 0.0046 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 127/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0022 - fn: 64.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 241.0000 - precision: 0.9269 - recall: 0.7902 - val_loss: 0.0044 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 128/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0020 - fn: 70.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 235.0000 - precision: 0.9289 - recall: 0.7705 - val_loss: 0.0045 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 129/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0022 - fn: 59.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 246.0000 - precision: 0.9389 - recall: 0.8066 - val_loss: 0.0045 - val_fn: 19.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 55.0000 - val_precision: 0.9322 - val_recall: 0.7432\n",
            "Epoch 130/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0021 - fn: 65.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 240.0000 - precision: 0.9412 - recall: 0.7869 - val_loss: 0.0046 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 58.0000 - val_precision: 0.8788 - val_recall: 0.7838\n",
            "Epoch 131/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0021 - fn: 62.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 243.0000 - precision: 0.9275 - recall: 0.7967 - val_loss: 0.0048 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 58.0000 - val_precision: 0.8788 - val_recall: 0.7838\n",
            "Epoch 132/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0023 - fn: 64.0000 - fp: 24.0000 - tn: 170555.0000 - tp: 241.0000 - precision: 0.9094 - recall: 0.7902 - val_loss: 0.0046 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 133/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0020 - fn: 63.0000 - fp: 17.0000 - tn: 170562.0000 - tp: 242.0000 - precision: 0.9344 - recall: 0.7934 - val_loss: 0.0046 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 134/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0020 - fn: 60.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 245.0000 - precision: 0.9316 - recall: 0.8033 - val_loss: 0.0047 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.7568\n",
            "Epoch 135/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0020 - fn: 61.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 244.0000 - precision: 0.9278 - recall: 0.8000 - val_loss: 0.0047 - val_fn: 17.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 57.0000 - val_precision: 0.8906 - val_recall: 0.7703\n",
            "Epoch 136/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0020 - fn: 57.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 248.0000 - precision: 0.9430 - recall: 0.8131 - val_loss: 0.0047 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 137/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0020 - fn: 61.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 244.0000 - precision: 0.9385 - recall: 0.8000 - val_loss: 0.0047 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 58.0000 - val_precision: 0.8788 - val_recall: 0.7838\n",
            "Epoch 138/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0020 - fn: 55.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 250.0000 - precision: 0.9434 - recall: 0.8197 - val_loss: 0.0045 - val_fn: 18.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.7568\n",
            "Epoch 139/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0022 - fn: 60.0000 - fp: 23.0000 - tn: 170556.0000 - tp: 245.0000 - precision: 0.9142 - recall: 0.8033 - val_loss: 0.0045 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 140/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0019 - fn: 56.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 249.0000 - precision: 0.9291 - recall: 0.8164 - val_loss: 0.0047 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 141/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0021 - fn: 56.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 249.0000 - precision: 0.9326 - recall: 0.8164 - val_loss: 0.0047 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 142/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0019 - fn: 61.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 244.0000 - precision: 0.9385 - recall: 0.8000 - val_loss: 0.0046 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 143/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 0.0019 - fn: 64.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 241.0000 - precision: 0.9377 - recall: 0.7902 - val_loss: 0.0048 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 144/300\n",
            "84/84 [==============================] - 1s 11ms/step - loss: 0.0019 - fn: 56.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 249.0000 - precision: 0.9432 - recall: 0.8164 - val_loss: 0.0045 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 145/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0020 - fn: 58.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 247.0000 - precision: 0.9427 - recall: 0.8098 - val_loss: 0.0047 - val_fn: 18.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.7568\n",
            "Epoch 146/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0019 - fn: 54.0000 - fp: 22.0000 - tn: 170557.0000 - tp: 251.0000 - precision: 0.9194 - recall: 0.8230 - val_loss: 0.0046 - val_fn: 19.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.7432\n",
            "Epoch 147/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0019 - fn: 62.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 243.0000 - precision: 0.9275 - recall: 0.7967 - val_loss: 0.0050 - val_fn: 17.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.7703\n",
            "Epoch 148/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0018 - fn: 59.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 246.0000 - precision: 0.9535 - recall: 0.8066 - val_loss: 0.0049 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 149/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0019 - fn: 58.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 247.0000 - precision: 0.9321 - recall: 0.8098 - val_loss: 0.0048 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 150/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0019 - fn: 59.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 246.0000 - precision: 0.9318 - recall: 0.8066 - val_loss: 0.0048 - val_fn: 18.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.7568\n",
            "Epoch 151/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0019 - fn: 59.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 246.0000 - precision: 0.9283 - recall: 0.8066 - val_loss: 0.0046 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 152/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0021 - fn: 59.0000 - fp: 24.0000 - tn: 170555.0000 - tp: 246.0000 - precision: 0.9111 - recall: 0.8066 - val_loss: 0.0045 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 153/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0020 - fn: 55.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 250.0000 - precision: 0.9328 - recall: 0.8197 - val_loss: 0.0049 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 154/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0017 - fn: 54.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 251.0000 - precision: 0.9544 - recall: 0.8230 - val_loss: 0.0053 - val_fn: 17.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.7703\n",
            "Epoch 155/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0019 - fn: 56.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 249.0000 - precision: 0.9432 - recall: 0.8164 - val_loss: 0.0049 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 58.0000 - val_precision: 0.8788 - val_recall: 0.7838\n",
            "Epoch 156/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0019 - fn: 56.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 249.0000 - precision: 0.9291 - recall: 0.8164 - val_loss: 0.0048 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 157/300\n",
            "84/84 [==============================] - 1s 11ms/step - loss: 0.0018 - fn: 54.0000 - fp: 17.0000 - tn: 170562.0000 - tp: 251.0000 - precision: 0.9366 - recall: 0.8230 - val_loss: 0.0048 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 158/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0018 - fn: 54.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 251.0000 - precision: 0.9436 - recall: 0.8230 - val_loss: 0.0047 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 159/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0018 - fn: 52.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 253.0000 - precision: 0.9405 - recall: 0.8295 - val_loss: 0.0049 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 160/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 0.0018 - fn: 54.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 251.0000 - precision: 0.9331 - recall: 0.8230 - val_loss: 0.0047 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 161/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0017 - fn: 54.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 251.0000 - precision: 0.9508 - recall: 0.8230 - val_loss: 0.0049 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 162/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0017 - fn: 54.0000 - fp: 17.0000 - tn: 170562.0000 - tp: 251.0000 - precision: 0.9366 - recall: 0.8230 - val_loss: 0.0049 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 163/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0016 - fn: 49.0000 - fp: 14.0000 - tn: 170565.0000 - tp: 256.0000 - precision: 0.9481 - recall: 0.8393 - val_loss: 0.0052 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 58.0000 - val_precision: 0.8788 - val_recall: 0.7838\n",
            "Epoch 164/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0016 - fn: 55.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 250.0000 - precision: 0.9506 - recall: 0.8197 - val_loss: 0.0049 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 165/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0018 - fn: 55.0000 - fp: 20.0000 - tn: 170559.0000 - tp: 250.0000 - precision: 0.9259 - recall: 0.8197 - val_loss: 0.0052 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 58.0000 - val_precision: 0.8788 - val_recall: 0.7838\n",
            "Epoch 166/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0018 - fn: 55.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 250.0000 - precision: 0.9294 - recall: 0.8197 - val_loss: 0.0047 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 167/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0017 - fn: 52.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 253.0000 - precision: 0.9511 - recall: 0.8295 - val_loss: 0.0050 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 58.0000 - val_precision: 0.8788 - val_recall: 0.7838\n",
            "Epoch 168/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0018 - fn: 59.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 246.0000 - precision: 0.9389 - recall: 0.8066 - val_loss: 0.0051 - val_fn: 15.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 59.0000 - val_precision: 0.8806 - val_recall: 0.7973\n",
            "Epoch 169/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0017 - fn: 55.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 250.0000 - precision: 0.9398 - recall: 0.8197 - val_loss: 0.0047 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 170/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0016 - fn: 52.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 253.0000 - precision: 0.9440 - recall: 0.8295 - val_loss: 0.0048 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 171/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0017 - fn: 55.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 250.0000 - precision: 0.9294 - recall: 0.8197 - val_loss: 0.0049 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 172/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0016 - fn: 52.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 253.0000 - precision: 0.9405 - recall: 0.8295 - val_loss: 0.0051 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 173/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0016 - fn: 55.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 250.0000 - precision: 0.9328 - recall: 0.8197 - val_loss: 0.0049 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 174/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0016 - fn: 48.0000 - fp: 19.0000 - tn: 170560.0000 - tp: 257.0000 - precision: 0.9312 - recall: 0.8426 - val_loss: 0.0051 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 175/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0016 - fn: 51.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 254.0000 - precision: 0.9585 - recall: 0.8328 - val_loss: 0.0050 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 176/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0016 - fn: 56.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 249.0000 - precision: 0.9504 - recall: 0.8164 - val_loss: 0.0052 - val_fn: 15.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 59.0000 - val_precision: 0.8806 - val_recall: 0.7973\n",
            "Epoch 177/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0016 - fn: 53.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 252.0000 - precision: 0.9403 - recall: 0.8262 - val_loss: 0.0048 - val_fn: 17.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.7703\n",
            "Epoch 178/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0016 - fn: 50.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 255.0000 - precision: 0.9410 - recall: 0.8361 - val_loss: 0.0048 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 179/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0015 - fn: 48.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 257.0000 - precision: 0.9414 - recall: 0.8426 - val_loss: 0.0052 - val_fn: 15.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 59.0000 - val_precision: 0.8806 - val_recall: 0.7973\n",
            "Epoch 180/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0016 - fn: 51.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 254.0000 - precision: 0.9549 - recall: 0.8328 - val_loss: 0.0050 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 181/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0016 - fn: 51.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 254.0000 - precision: 0.9442 - recall: 0.8328 - val_loss: 0.0046 - val_fn: 17.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.7703\n",
            "Epoch 182/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0016 - fn: 54.0000 - fp: 17.0000 - tn: 170562.0000 - tp: 251.0000 - precision: 0.9366 - recall: 0.8230 - val_loss: 0.0047 - val_fn: 17.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.7703\n",
            "Epoch 183/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0014 - fn: 54.0000 - fp: 14.0000 - tn: 170565.0000 - tp: 251.0000 - precision: 0.9472 - recall: 0.8230 - val_loss: 0.0048 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 184/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0016 - fn: 49.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 256.0000 - precision: 0.9343 - recall: 0.8393 - val_loss: 0.0048 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 185/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0015 - fn: 48.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 257.0000 - precision: 0.9554 - recall: 0.8426 - val_loss: 0.0052 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 186/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0014 - fn: 43.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 262.0000 - precision: 0.9458 - recall: 0.8590 - val_loss: 0.0050 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 187/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0015 - fn: 53.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 252.0000 - precision: 0.9509 - recall: 0.8262 - val_loss: 0.0053 - val_fn: 15.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 59.0000 - val_precision: 0.8806 - val_recall: 0.7973\n",
            "Epoch 188/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0015 - fn: 47.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 258.0000 - precision: 0.9520 - recall: 0.8459 - val_loss: 0.0050 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 189/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0013 - fn: 46.0000 - fp: 14.0000 - tn: 170565.0000 - tp: 259.0000 - precision: 0.9487 - recall: 0.8492 - val_loss: 0.0052 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 190/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0014 - fn: 44.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 261.0000 - precision: 0.9526 - recall: 0.8557 - val_loss: 0.0052 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 191/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0015 - fn: 50.0000 - fp: 20.0000 - tn: 170559.0000 - tp: 255.0000 - precision: 0.9273 - recall: 0.8361 - val_loss: 0.0051 - val_fn: 18.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 56.0000 - val_precision: 0.9333 - val_recall: 0.7568\n",
            "Epoch 192/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - fn: 50.0000 - fp: 14.0000 - tn: 170565.0000 - tp: 255.0000 - precision: 0.9480 - recall: 0.8361 - val_loss: 0.0052 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 193/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0014 - fn: 44.0000 - fp: 17.0000 - tn: 170562.0000 - tp: 261.0000 - precision: 0.9388 - recall: 0.8557 - val_loss: 0.0050 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 194/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0014 - fn: 48.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 257.0000 - precision: 0.9519 - recall: 0.8426 - val_loss: 0.0052 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 195/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0014 - fn: 44.0000 - fp: 17.0000 - tn: 170562.0000 - tp: 261.0000 - precision: 0.9388 - recall: 0.8557 - val_loss: 0.0050 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 196/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0014 - fn: 44.0000 - fp: 14.0000 - tn: 170565.0000 - tp: 261.0000 - precision: 0.9491 - recall: 0.8557 - val_loss: 0.0049 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 197/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0015 - fn: 43.0000 - fp: 9.0000 - tn: 170570.0000 - tp: 262.0000 - precision: 0.9668 - recall: 0.8590 - val_loss: 0.0055 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 58.0000 - val_precision: 0.8788 - val_recall: 0.7838\n",
            "Epoch 198/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0013 - fn: 47.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 258.0000 - precision: 0.9520 - recall: 0.8459 - val_loss: 0.0057 - val_fn: 16.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 58.0000 - val_precision: 0.8788 - val_recall: 0.7838\n",
            "Epoch 199/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0012 - fn: 41.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 264.0000 - precision: 0.9565 - recall: 0.8656 - val_loss: 0.0054 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 200/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0014 - fn: 46.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 259.0000 - precision: 0.9522 - recall: 0.8492 - val_loss: 0.0053 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 201/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0016 - fn: 55.0000 - fp: 18.0000 - tn: 170561.0000 - tp: 250.0000 - precision: 0.9328 - recall: 0.8197 - val_loss: 0.0055 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 202/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0015 - fn: 41.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 264.0000 - precision: 0.9565 - recall: 0.8656 - val_loss: 0.0053 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 203/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0013 - fn: 43.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 262.0000 - precision: 0.9597 - recall: 0.8590 - val_loss: 0.0053 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 204/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0016 - fn: 48.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 257.0000 - precision: 0.9414 - recall: 0.8426 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 59.0000 - val_precision: 0.8806 - val_recall: 0.7973\n",
            "Epoch 205/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0013 - fn: 48.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 257.0000 - precision: 0.9590 - recall: 0.8426 - val_loss: 0.0054 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 206/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0014 - fn: 46.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 259.0000 - precision: 0.9522 - recall: 0.8492 - val_loss: 0.0053 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 207/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0012 - fn: 43.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 262.0000 - precision: 0.9562 - recall: 0.8590 - val_loss: 0.0053 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 208/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - fn: 41.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 264.0000 - precision: 0.9462 - recall: 0.8656 - val_loss: 0.0052 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 209/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0014 - fn: 47.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 258.0000 - precision: 0.9556 - recall: 0.8459 - val_loss: 0.0056 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 210/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0014 - fn: 43.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 262.0000 - precision: 0.9458 - recall: 0.8590 - val_loss: 0.0053 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 211/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0012 - fn: 38.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 267.0000 - precision: 0.9604 - recall: 0.8754 - val_loss: 0.0054 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 212/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0012 - fn: 42.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 263.0000 - precision: 0.9634 - recall: 0.8623 - val_loss: 0.0054 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 213/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0013 - fn: 44.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 261.0000 - precision: 0.9560 - recall: 0.8557 - val_loss: 0.0053 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 214/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0013 - fn: 41.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 264.0000 - precision: 0.9635 - recall: 0.8656 - val_loss: 0.0051 - val_fn: 18.0000 - val_fp: 3.0000 - val_tn: 42644.0000 - val_tp: 56.0000 - val_precision: 0.9492 - val_recall: 0.7568\n",
            "Epoch 215/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0012 - fn: 37.0000 - fp: 14.0000 - tn: 170565.0000 - tp: 268.0000 - precision: 0.9504 - recall: 0.8787 - val_loss: 0.0054 - val_fn: 17.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.7703\n",
            "Epoch 216/300\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0013 - fn: 44.0000 - fp: 17.0000 - tn: 170562.0000 - tp: 261.0000 - precision: 0.9388 - recall: 0.8557 - val_loss: 0.0052 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 217/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0012 - fn: 39.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 266.0000 - precision: 0.9568 - recall: 0.8721 - val_loss: 0.0055 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 218/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0011 - fn: 40.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 265.0000 - precision: 0.9532 - recall: 0.8689 - val_loss: 0.0054 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 219/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0012 - fn: 44.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 261.0000 - precision: 0.9631 - recall: 0.8557 - val_loss: 0.0055 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 220/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0012 - fn: 44.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 261.0000 - precision: 0.9526 - recall: 0.8557 - val_loss: 0.0055 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 221/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0011 - fn: 36.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 269.0000 - precision: 0.9539 - recall: 0.8820 - val_loss: 0.0058 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 222/300\n",
            "84/84 [==============================] - 1s 11ms/step - loss: 0.0012 - fn: 40.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 265.0000 - precision: 0.9431 - recall: 0.8689 - val_loss: 0.0055 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 223/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0012 - fn: 40.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 265.0000 - precision: 0.9567 - recall: 0.8689 - val_loss: 0.0054 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 224/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0012 - fn: 38.0000 - fp: 7.0000 - tn: 170572.0000 - tp: 267.0000 - precision: 0.9745 - recall: 0.8754 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 225/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 0.0012 - fn: 44.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 261.0000 - precision: 0.9422 - recall: 0.8557 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 226/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0010 - fn: 33.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 272.0000 - precision: 0.9611 - recall: 0.8918 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 227/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0013 - fn: 48.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 257.0000 - precision: 0.9625 - recall: 0.8426 - val_loss: 0.0055 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 228/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0012 - fn: 39.0000 - fp: 14.0000 - tn: 170565.0000 - tp: 266.0000 - precision: 0.9500 - recall: 0.8721 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 59.0000 - val_precision: 0.8806 - val_recall: 0.7973\n",
            "Epoch 229/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0011 - fn: 37.0000 - fp: 7.0000 - tn: 170572.0000 - tp: 268.0000 - precision: 0.9745 - recall: 0.8787 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 230/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0011 - fn: 40.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 265.0000 - precision: 0.9636 - recall: 0.8689 - val_loss: 0.0054 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 231/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0012 - fn: 40.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 265.0000 - precision: 0.9636 - recall: 0.8689 - val_loss: 0.0055 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 232/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0010 - fn: 38.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 267.0000 - precision: 0.9604 - recall: 0.8754 - val_loss: 0.0056 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 233/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0011 - fn: 39.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 266.0000 - precision: 0.9603 - recall: 0.8721 - val_loss: 0.0054 - val_fn: 15.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 59.0000 - val_precision: 0.9365 - val_recall: 0.7973\n",
            "Epoch 234/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0011 - fn: 41.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 264.0000 - precision: 0.9635 - recall: 0.8656 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 235/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0010 - fn: 36.0000 - fp: 6.0000 - tn: 170573.0000 - tp: 269.0000 - precision: 0.9782 - recall: 0.8820 - val_loss: 0.0056 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 236/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.6177e-04 - fn: 30.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 275.0000 - precision: 0.9549 - recall: 0.9016 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 237/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0011 - fn: 38.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 267.0000 - precision: 0.9639 - recall: 0.8754 - val_loss: 0.0059 - val_fn: 15.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 59.0000 - val_precision: 0.9365 - val_recall: 0.7973\n",
            "Epoch 238/300\n",
            "84/84 [==============================] - 1s 11ms/step - loss: 0.0012 - fn: 39.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 266.0000 - precision: 0.9638 - recall: 0.8721 - val_loss: 0.0058 - val_fn: 16.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 58.0000 - val_precision: 0.9062 - val_recall: 0.7838\n",
            "Epoch 239/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 0.0012 - fn: 38.0000 - fp: 8.0000 - tn: 170571.0000 - tp: 267.0000 - precision: 0.9709 - recall: 0.8754 - val_loss: 0.0056 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 240/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 9.8640e-04 - fn: 42.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 263.0000 - precision: 0.9564 - recall: 0.8623 - val_loss: 0.0057 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 241/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 0.0011 - fn: 34.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 271.0000 - precision: 0.9576 - recall: 0.8885 - val_loss: 0.0059 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 242/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0010 - fn: 34.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 271.0000 - precision: 0.9576 - recall: 0.8885 - val_loss: 0.0058 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 243/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0010 - fn: 35.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 270.0000 - precision: 0.9574 - recall: 0.8852 - val_loss: 0.0058 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 244/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0011 - fn: 33.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 272.0000 - precision: 0.9577 - recall: 0.8918 - val_loss: 0.0055 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 245/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0010 - fn: 36.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 269.0000 - precision: 0.9573 - recall: 0.8820 - val_loss: 0.0055 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 246/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0011 - fn: 36.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 269.0000 - precision: 0.9472 - recall: 0.8820 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 247/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.7182e-04 - fn: 32.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 273.0000 - precision: 0.9647 - recall: 0.8951 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 248/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0010 - fn: 36.0000 - fp: 9.0000 - tn: 170570.0000 - tp: 269.0000 - precision: 0.9676 - recall: 0.8820 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 249/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 9.7617e-04 - fn: 30.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 275.0000 - precision: 0.9549 - recall: 0.9016 - val_loss: 0.0059 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 250/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.9062e-04 - fn: 33.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 272.0000 - precision: 0.9611 - recall: 0.8918 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 251/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.5826e-04 - fn: 35.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 270.0000 - precision: 0.9609 - recall: 0.8852 - val_loss: 0.0058 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 252/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0011 - fn: 36.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 269.0000 - precision: 0.9439 - recall: 0.8820 - val_loss: 0.0061 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 253/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.8317e-04 - fn: 31.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 274.0000 - precision: 0.9547 - recall: 0.8984 - val_loss: 0.0058 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 254/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 7.8227e-04 - fn: 29.0000 - fp: 5.0000 - tn: 170574.0000 - tp: 276.0000 - precision: 0.9822 - recall: 0.9049 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 255/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0011 - fn: 36.0000 - fp: 9.0000 - tn: 170570.0000 - tp: 269.0000 - precision: 0.9676 - recall: 0.8820 - val_loss: 0.0055 - val_fn: 15.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 59.0000 - val_precision: 0.9365 - val_recall: 0.7973\n",
            "Epoch 256/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0010 - fn: 37.0000 - fp: 8.0000 - tn: 170571.0000 - tp: 268.0000 - precision: 0.9710 - recall: 0.8787 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 257/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 8.6925e-04 - fn: 30.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 275.0000 - precision: 0.9582 - recall: 0.9016 - val_loss: 0.0059 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 258/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.7382e-04 - fn: 34.0000 - fp: 9.0000 - tn: 170570.0000 - tp: 271.0000 - precision: 0.9679 - recall: 0.8885 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 259/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.4254e-04 - fn: 31.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 274.0000 - precision: 0.9614 - recall: 0.8984 - val_loss: 0.0058 - val_fn: 16.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 58.0000 - val_precision: 0.9355 - val_recall: 0.7838\n",
            "Epoch 260/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0011 - fn: 31.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 274.0000 - precision: 0.9614 - recall: 0.8984 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 261/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.8764e-04 - fn: 33.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 272.0000 - precision: 0.9577 - recall: 0.8918 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 262/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.6286e-04 - fn: 39.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 266.0000 - precision: 0.9568 - recall: 0.8721 - val_loss: 0.0059 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 263/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.4258e-04 - fn: 32.0000 - fp: 7.0000 - tn: 170572.0000 - tp: 273.0000 - precision: 0.9750 - recall: 0.8951 - val_loss: 0.0059 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 264/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.6718e-04 - fn: 35.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 270.0000 - precision: 0.9574 - recall: 0.8852 - val_loss: 0.0058 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 265/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.0553e-04 - fn: 32.0000 - fp: 8.0000 - tn: 170571.0000 - tp: 273.0000 - precision: 0.9715 - recall: 0.8951 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 59.0000 - val_precision: 0.9365 - val_recall: 0.7973\n",
            "Epoch 266/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0012 - fn: 36.0000 - fp: 16.0000 - tn: 170563.0000 - tp: 269.0000 - precision: 0.9439 - recall: 0.8820 - val_loss: 0.0057 - val_fn: 15.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 59.0000 - val_precision: 0.9365 - val_recall: 0.7973\n",
            "Epoch 267/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.4237e-04 - fn: 34.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 271.0000 - precision: 0.9576 - recall: 0.8885 - val_loss: 0.0055 - val_fn: 15.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 59.0000 - val_precision: 0.9365 - val_recall: 0.7973\n",
            "Epoch 268/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 0.0011 - fn: 37.0000 - fp: 14.0000 - tn: 170565.0000 - tp: 268.0000 - precision: 0.9504 - recall: 0.8787 - val_loss: 0.0056 - val_fn: 17.0000 - val_fp: 4.0000 - val_tn: 42643.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.7703\n",
            "Epoch 269/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.4028e-04 - fn: 33.0000 - fp: 8.0000 - tn: 170571.0000 - tp: 272.0000 - precision: 0.9714 - recall: 0.8918 - val_loss: 0.0059 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 270/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 8.4702e-04 - fn: 30.0000 - fp: 7.0000 - tn: 170572.0000 - tp: 275.0000 - precision: 0.9752 - recall: 0.9016 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 271/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 7.3682e-04 - fn: 23.0000 - fp: 5.0000 - tn: 170574.0000 - tp: 282.0000 - precision: 0.9826 - recall: 0.9246 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 272/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 7.7376e-04 - fn: 29.0000 - fp: 8.0000 - tn: 170571.0000 - tp: 276.0000 - precision: 0.9718 - recall: 0.9049 - val_loss: 0.0059 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 273/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 8.5393e-04 - fn: 33.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 272.0000 - precision: 0.9577 - recall: 0.8918 - val_loss: 0.0057 - val_fn: 16.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 58.0000 - val_precision: 0.8923 - val_recall: 0.7838\n",
            "Epoch 274/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 9.5351e-04 - fn: 36.0000 - fp: 13.0000 - tn: 170566.0000 - tp: 269.0000 - precision: 0.9539 - recall: 0.8820 - val_loss: 0.0061 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 275/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.5501e-04 - fn: 32.0000 - fp: 8.0000 - tn: 170571.0000 - tp: 273.0000 - precision: 0.9715 - recall: 0.8951 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 276/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.5571e-04 - fn: 31.0000 - fp: 8.0000 - tn: 170571.0000 - tp: 274.0000 - precision: 0.9716 - recall: 0.8984 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 277/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.7484e-04 - fn: 29.0000 - fp: 8.0000 - tn: 170571.0000 - tp: 276.0000 - precision: 0.9718 - recall: 0.9049 - val_loss: 0.0059 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 278/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.5362e-04 - fn: 28.0000 - fp: 9.0000 - tn: 170570.0000 - tp: 277.0000 - precision: 0.9685 - recall: 0.9082 - val_loss: 0.0065 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 279/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0011 - fn: 37.0000 - fp: 14.0000 - tn: 170565.0000 - tp: 268.0000 - precision: 0.9504 - recall: 0.8787 - val_loss: 0.0060 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 280/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.9340e-04 - fn: 33.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 272.0000 - precision: 0.9577 - recall: 0.8918 - val_loss: 0.0064 - val_fn: 15.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 59.0000 - val_precision: 0.8806 - val_recall: 0.7973\n",
            "Epoch 281/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 7.7783e-04 - fn: 23.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 282.0000 - precision: 0.9658 - recall: 0.9246 - val_loss: 0.0063 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 282/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 7.1288e-04 - fn: 25.0000 - fp: 8.0000 - tn: 170571.0000 - tp: 280.0000 - precision: 0.9722 - recall: 0.9180 - val_loss: 0.0062 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 283/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.9391e-04 - fn: 33.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 272.0000 - precision: 0.9577 - recall: 0.8918 - val_loss: 0.0063 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 284/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.5635e-04 - fn: 34.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 271.0000 - precision: 0.9610 - recall: 0.8885 - val_loss: 0.0061 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 285/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 7.7097e-04 - fn: 28.0000 - fp: 9.0000 - tn: 170570.0000 - tp: 277.0000 - precision: 0.9685 - recall: 0.9082 - val_loss: 0.0067 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 286/300\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 8.3190e-04 - fn: 26.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 279.0000 - precision: 0.9654 - recall: 0.9148 - val_loss: 0.0063 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 287/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 8.0718e-04 - fn: 28.0000 - fp: 7.0000 - tn: 170572.0000 - tp: 277.0000 - precision: 0.9754 - recall: 0.9082 - val_loss: 0.0062 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 288/300\n",
            "84/84 [==============================] - 1s 14ms/step - loss: 8.2941e-04 - fn: 28.0000 - fp: 9.0000 - tn: 170570.0000 - tp: 277.0000 - precision: 0.9685 - recall: 0.9082 - val_loss: 0.0064 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 289/300\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 7.9312e-04 - fn: 29.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 276.0000 - precision: 0.9650 - recall: 0.9049 - val_loss: 0.0064 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 290/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 7.7543e-04 - fn: 32.0000 - fp: 7.0000 - tn: 170572.0000 - tp: 273.0000 - precision: 0.9750 - recall: 0.8951 - val_loss: 0.0061 - val_fn: 15.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 59.0000 - val_precision: 0.9219 - val_recall: 0.7973\n",
            "Epoch 291/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.9427e-04 - fn: 31.0000 - fp: 11.0000 - tn: 170568.0000 - tp: 274.0000 - precision: 0.9614 - recall: 0.8984 - val_loss: 0.0067 - val_fn: 15.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 59.0000 - val_precision: 0.8806 - val_recall: 0.7973\n",
            "Epoch 292/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 7.8379e-04 - fn: 29.0000 - fp: 10.0000 - tn: 170569.0000 - tp: 276.0000 - precision: 0.9650 - recall: 0.9049 - val_loss: 0.0065 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 293/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.6586e-04 - fn: 33.0000 - fp: 9.0000 - tn: 170570.0000 - tp: 272.0000 - precision: 0.9680 - recall: 0.8918 - val_loss: 0.0065 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 294/300\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 7.6760e-04 - fn: 23.0000 - fp: 9.0000 - tn: 170570.0000 - tp: 282.0000 - precision: 0.9691 - recall: 0.9246 - val_loss: 0.0062 - val_fn: 16.0000 - val_fp: 5.0000 - val_tn: 42642.0000 - val_tp: 58.0000 - val_precision: 0.9206 - val_recall: 0.7838\n",
            "Epoch 295/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 6.9221e-04 - fn: 23.0000 - fp: 7.0000 - tn: 170572.0000 - tp: 282.0000 - precision: 0.9758 - recall: 0.9246 - val_loss: 0.0068 - val_fn: 15.0000 - val_fp: 7.0000 - val_tn: 42640.0000 - val_tp: 59.0000 - val_precision: 0.8939 - val_recall: 0.7973\n",
            "Epoch 296/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.6383e-04 - fn: 35.0000 - fp: 12.0000 - tn: 170567.0000 - tp: 270.0000 - precision: 0.9574 - recall: 0.8852 - val_loss: 0.0067 - val_fn: 15.0000 - val_fp: 8.0000 - val_tn: 42639.0000 - val_tp: 59.0000 - val_precision: 0.8806 - val_recall: 0.7973\n",
            "Epoch 297/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 7.9822e-04 - fn: 29.0000 - fp: 8.0000 - tn: 170571.0000 - tp: 276.0000 - precision: 0.9718 - recall: 0.9049 - val_loss: 0.0066 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 298/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.2822e-04 - fn: 33.0000 - fp: 15.0000 - tn: 170564.0000 - tp: 272.0000 - precision: 0.9477 - recall: 0.8918 - val_loss: 0.0065 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 299/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 7.8140e-04 - fn: 27.0000 - fp: 9.0000 - tn: 170570.0000 - tp: 278.0000 - precision: 0.9686 - recall: 0.9115 - val_loss: 0.0065 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n",
            "Epoch 300/300\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 7.7969e-04 - fn: 29.0000 - fp: 7.0000 - tn: 170572.0000 - tp: 276.0000 - precision: 0.9753 - recall: 0.9049 - val_loss: 0.0063 - val_fn: 15.0000 - val_fp: 6.0000 - val_tn: 42641.0000 - val_tp: 59.0000 - val_precision: 0.9077 - val_recall: 0.7973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print(score)\n",
        "print('Precision: ', score[-2])\n",
        "print('Recall: ', score[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImQRy_lwWtQA",
        "outputId": "288db818-a854-46fb-f2ce-8993139a9eda"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2226/2226 [==============================] - 8s 4ms/step - loss: 0.0043 - fn: 26.0000 - fp: 13.0000 - tn: 71076.0000 - tp: 87.0000 - precision: 0.8700 - recall: 0.7699\n",
            "[0.004324023611843586, 26.0, 13.0, 71076.0, 87.0, 0.8700000047683716, 0.769911527633667]\n",
            "Precision:  0.8700000047683716\n",
            "Recall:  0.769911527633667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost Classifier"
      ],
      "metadata": {
        "id": "nYFtE11Ubhtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_clf = XGBClassifier()\n",
        "xgb_clf.fit(X_train, y_train, eval_metric='aucpr')\n",
        "\n",
        "y_train_pred = xgb_clf.predict(X_train)\n",
        "y_test_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "print_score(y_train, y_train_pred, train=True)\n",
        "print_score(y_test, y_test_pred, train=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVW-O67sblEF",
        "outputId": "f027ea5c-672c-4195-b388-be2bd394a09c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Result:\n",
            "================================================\n",
            "_______________________________________________\n",
            "Classification Report:\n",
            "                  0      1  accuracy  macro avg  weighted avg\n",
            "precision       1.0    1.0       1.0        1.0           1.0\n",
            "recall          1.0    1.0       1.0        1.0           1.0\n",
            "f1-score        1.0    1.0       1.0        1.0           1.0\n",
            "support    170579.0  305.0       1.0   170884.0      170884.0\n",
            "_______________________________________________\n",
            "Confusion Matrix: \n",
            " [[170579      0]\n",
            " [     0    305]]\n",
            "\n",
            "Test Result:\n",
            "================================================\n",
            "_______________________________________________\n",
            "Classification Report:\n",
            "                      0           1  accuracy     macro avg  weighted avg\n",
            "precision      0.999677    0.909091  0.999551      0.954384      0.999533\n",
            "recall         0.999873    0.796460  0.999551      0.898167      0.999551\n",
            "f1-score       0.999775    0.849057  0.999551      0.924416      0.999536\n",
            "support    71089.000000  113.000000  0.999551  71202.000000  71202.000000\n",
            "_______________________________________________\n",
            "Confusion Matrix: \n",
            " [[71080     9]\n",
            " [   23    90]]\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8OXN8kuDsJgy",
        "964pU_GGsJk8",
        "ymd6LpyNsJ9B",
        "IkpO6tI5sKRw",
        "UyCe0Am6sabn"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}